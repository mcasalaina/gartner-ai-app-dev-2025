{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restaurant Agent Simulator-Based Evaluation\n",
    "\n",
    "This notebook evaluates the Scheibmeir's restaurant agent using Azure AI Evaluation's simulator capabilities to generate contextually relevant test queries automatically.\n",
    "Instead of using pre-defined queries, it uses AI to simulate realistic customer interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from typing import Dict, List, Optional, Any\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    GroundednessEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    AzureOpenAIModelConfiguration\n",
    ")\n",
    "from azure.ai.evaluation.simulator import Simulator\n",
    "from azure.ai.agents import AgentsClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "metadata": {},\n",
   "outputs": [],\n",
   "source": [
    "# Configuration from environment variables\n",
    "RESTAURANT_ASSISTANT_ID = os.getenv(\"RESTAURANT_ASSISTANT_ID\")\n",
    "RESTAURANT_ASSISTANT_PROJECT = os.getenv(\"RESTAURANT_ASSISTANT_PROJECT\")\n",
    "\n",
    "# Azure OpenAI configuration for evaluators and simulator\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"DEEP_RESEARCH_PROJECT_ENDPOINT\").replace(\"/api/projects/deep-research-demo-project\", \"\")\n",
    "AZURE_OPENAI_API_VERSION = \"2024-10-21\"\n",
    "AGENT_MODEL_DEPLOYMENT_NAME = os.getenv(\"AGENT_MODEL_DEPLOYMENT_NAME\")\n",
    "\n",
    "# Azure AI project configuration\n",
    "AZURE_SUBSCRIPTION_ID = os.getenv(\"AZURE_SUBSCRIPTION_ID\")\n",
    "AZURE_RESOURCE_GROUP_NAME = os.getenv(\"AZURE_RESOURCE_GROUP_NAME\")\n",
    "AZURE_PROJECT_NAME = os.getenv(\"AZURE_PROJECT_NAME\")\n",
    "\n",
    "print(f\"Restaurant Assistant ID: {RESTAURANT_ASSISTANT_ID}\")\n",
    "print(f\"Azure OpenAI Endpoint: {AZURE_OPENAI_ENDPOINT}\")\n",
    "print(f\"Model Deployment: {AGENT_MODEL_DEPLOYMENT_NAME}\")\n",
    "print(f\"Project: {AZURE_PROJECT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "metadata": {},\n",
   "outputs": [],\n",
   "source": [
    "# Initialize Azure credential\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Initialize the Agents client\n",
    "agents_client = AgentsClient(\n",
    "    endpoint=RESTAURANT_ASSISTANT_PROJECT,\n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "print(\"Clients initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Model for Simulator and Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Configure the model for AI-assisted evaluators and simulator\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    azure_deployment=AGENT_MODEL_DEPLOYMENT_NAME,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "print(\"Model configuration created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Target Function for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "metadata": {},\n",
   "outputs": [],\n",
   "source": [
    "async def query_restaurant_agent_async(query: str) -> dict:\n",
    "    \"\"\"\n",
    "    Async function to query the restaurant agent and return the response.\n",
    "    This function will be used by the Azure AI evaluation framework.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a thread and run in one step\n",
    "        result = agents_client.create_thread_and_process_run(\n",
    "            agent_id=RESTAURANT_ASSISTANT_ID,\n",
    "            thread={\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": query\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if result.status.value == \"completed\":\n",
    "            # Get messages from the thread\n",
    "            messages = agents_client.messages.list(thread_id=result.thread_id)\n",
    "            \n",
    "            # Find the agent's response\n",
    "            for msg in messages:\n",
    "                if msg.role.value == \"agent\":\n",
    "                    return {\n",
    "                        \"response\": msg.content[0].text.value,\n",
    "                        \"query\": query\n",
    "                    }\n",
    "        \n",
    "        return {\n",
    "            \"response\": f\"Agent run failed with status: {result.status.value}\",\n",
    "            \"query\": query\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"response\": f\"Error querying agent: {str(e)}\",\n",
    "            \"query\": query\n",
    "        }\n",
    "\n",
    "print(\"Restaurant agent target function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
   "metadata": {},\n",
   "outputs": [],\n",
    "source": [
    "# Initialize the simulator\n",
    "simulator = Simulator(model_config=model_config)\n",
    "\n",
    "print(\"Simulator initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},\n",
    "metadata": {},\n",
   "source": [\n",
    "## Generate Simulated Conversations\\n\",\n",
    "\\n\",\n",
    "The simulator will generate realistic restaurant customer queries based on the context we provide.\"\n",
   ]\n",
  },\n",
  {\n",
   \"cell_type\": \"code\",\n",
   \"execution_count\": null,\n",
   \"metadata\": {},\n",
   \"outputs\": [],\n",
   \"source\": [\n",
    "    \"# Define the context for the simulator\\n\",\n",
    "    \"restaurant_context = \\\"\\\"\\\"\\n\",\n",
    "    \"You are simulating customers interacting with Scheibmeir's Steaks, Snacks and Sticks restaurant assistant.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Restaurant Information:\\n\",\n",
    "    \"- Name: Scheibmeir's Steaks, Snacks and Sticks\\n\",\n",
    "    \"- Location: 340 Jefferson St., San Francisco, CA\\n\",\n",
    "    \"- Founded: 1996 in Fort Collins, Colorado by Chef Jim Scheibmeir\\n\",\n",
    "    \"- Cuisine: Steaks, American appetizers, Chinese-inspired dishes, and Jello salads\\n\",\n",
    "    \"- Hours: Mon-Thu 4-10pm, Fri 4-11pm, Sat 12-11pm, Sun 12-9pm\\n\",\n",
    "    \"- Phone: (415) 555-STEAK\\n\",\n",
    "    \"- Email: info@scheibmeirs.com\\n\",\n",
    "    \"\\n\",\n",
    "    \"Generate diverse customer queries including:\\n\",\n",
    "    \"- Questions about menu items and prices\\n\",\n",
    "    \"- Requests for restaurant information (hours, location, contact)\\n\",\n",
    "    \"- Reservation and dining inquiries\\n\",\n",
    "    \"- Questions about restaurant history and chef\\n\",\n",
    "    \"- Some off-topic questions to test the agent's focus\\n\",\n",
    "    \"\\\"\\\"\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Restaurant context defined for simulator!\\\")\"\n",
   ]\n",
  },\n",
  {\n",
   \"cell_type\": \"code\",\n",
   \"execution_count\": null,\n",
   \"metadata\": {},\n",
   \"outputs\": [],\n",
   \"source\": [\n",
    \"    \"# Generate simulated conversations\\n\",\n",
    \"    \"print(\\\"Generating simulated restaurant customer queries...\\\")\\n\",\n",
    \"    \"print(\\\"This may take a few minutes.\\\")\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"simulated_conversations = await simulator.generate_async(\\n\",\n",
    \"    \"    target=query_restaurant_agent_async,\\n\",\n",
    \"    \"    text_inputs=[restaurant_context],\\n\",\n",
    \"    \"    num_queries=50,  # Generate 50 simulated queries\\n\",\n",
    \"    \"    max_conversation_turns=1,  # Single turn conversations\\n\",\n",
    \"    \")\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"print(f\\\"Generated {len(simulated_conversations)} simulated conversations!\\\")\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"# Save simulated data for evaluation\\n\",\n",
    \"    \"simulated_data = []\\n\",\n",
    \"    \"for i, conversation in enumerate(simulated_conversations):\\n\",\n",
    \"    \"    simulated_data.append({\\n\",\n",
    \"    \"        \\\"query\\\": conversation.get('query', f'Simulated query {i+1}'),\\n\",\n",
    \"    \"        \\\"response\\\": conversation.get('response', 'No response'),\\n\",\n",
    \"    \"        \\\"conversation_id\\\": f\\\"sim_{i+1}\\\"\\n\",\n",
    \"    \"    })\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"# Save to JSONL file\\n\",\n",
    \"    \"with open(\\\"simulated_queries.jsonl\\\", \\\"w\\\") as f:\\n\",\n",
    \"    \"    for item in simulated_data:\\n\",\n",
    \"    \"        f.write(json.dumps(item) + \\\"\\\\n\\\")\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"print(\\\"Simulated data saved to simulated_queries.jsonl\\\")\"\n",
   ]\n",
  },\n",
  {\n",
   \"cell_type\": \"markdown\",\n",
   \"metadata\": {},\n",
   \"source\": [\n",
    \"    \"## Preview Generated Queries\"\n",
   ]\n",
  },\n",
  {\n",
   \"cell_type\": \"code\",\n",
   \"execution_count\": null,\n",
   \"metadata\": {},\n",
   \"outputs\": [],\n",
   \"source\": [\n",
    \"    \"# Show sample generated queries\\n\",\n",
    \"    \"print(\\\"Sample Generated Queries:\\\")\\n\",\n",
    \"    \"print(\\\"=\\\" * 50)\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"for i in range(min(10, len(simulated_data))):\\n\",\n",
    \"    \"    query = simulated_data[i]['query']\\n\",\n",
    \"    \"    response = simulated_data[i]['response']\\n\",\n",
    \"    \"    print(f\\\"\\\\n{i+1}. Query: {query}\\\")\\n\",\n",
    \"    \"    print(f\\\"   Response: {response[:150]}{'...' if len(response) > 150 else ''}\\\")\\n\",\n",
    \"    \"    print(\\\"-\\\" * 60)\"\n",
   ]\n",
  },\n",
  {\n",
   \"cell_type\": \"markdown\",\n",
   \"metadata\": {},\n",
   \"source\": [\n",
    \"    \"## Configure Evaluators\"\n",
   ]\n",
  },\n",
  {\n",
   \"cell_type\": \"code\",\n",
   \"execution_count\": null,\n",
   \"metadata\": {},\n",
   \"outputs\": [],\n",
   \"source\": [\n",
    \"    \"# Initialize evaluators\\n\",\n",
    \"    \"groundedness_evaluator = GroundednessEvaluator(model_config=model_config)\\n\",\n",
    \"    \"relevance_evaluator = RelevanceEvaluator(model_config=model_config)\\n\",\n",
    \"    \"coherence_evaluator = CoherenceEvaluator(model_config=model_config)\\n\",\n",
    \"    \"fluency_evaluator = FluencyEvaluator(model_config=model_config)\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"print(\\\"Evaluators configured successfully!\\\")\"\n",
   ]\n",
  },\n",
  {\n",
   \"cell_type\": \"markdown\",\n",
   \"metadata\": {},\n",
   \"source\": [\n",
    \"    \"## Run Evaluation on Simulated Data\"\n",
   ]\n",
  },\n",
  {\n",
   \"cell_type\": \"code\",\n",
   \"execution_count\": null,\n",
   \"metadata\": {},\n",
   \"outputs\": [],\n",
   \"source\": [\n",
    \"    \"# Azure AI project configuration\\n\",\n",
    \"    \"azure_ai_project = {\\n\",\n",
    \"    \"    \\\"subscription_id\\\": AZURE_SUBSCRIPTION_ID,\\n\",\n",
    \"    \"    \\\"project_name\\\": AZURE_PROJECT_NAME,\\n\",\n",
    \"    \"    \\\"resource_group_name\\\": AZURE_RESOURCE_GROUP_NAME,\\n\",\n",
    \"    \"}\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"# Run evaluation on simulated data\\n\",\n",
    \"    \"print(\\\"Starting evaluation on simulated data...\\\")\\n\",\n",
    \"    \"print(\\\"This may take several minutes.\\\")\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"evaluation_result = evaluate(\\n\",\n",
    \"    \"    data=\\\"simulated_queries.jsonl\\\",\\n\",\n",
    \"    \"    target=query_restaurant_agent_async,\\n\",\n",
    \"    \"    evaluators={\\n\",\n",
    \"    \"        \\\"groundedness\\\": groundedness_evaluator,\\n\",\n",
    \"    \"        \\\"relevance\\\": relevance_evaluator,\\n\",\n",
    \"    \"        \\\"coherence\\\": coherence_evaluator,\\n\",\n",
    \"    \"        \\\"fluency\\\": fluency_evaluator,\\n\",\n",
    \"    \"    },\\n\",\n",
    \"    \"    azure_ai_project=azure_ai_project,\\n\",\n",
    \"    \")\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"print(\\\"Evaluation completed!\\\")\\n\",\n",
    \"    \"print(f\\\"Azure AI Foundry Studio URL: {evaluation_result.get('studio_url')}\\\")\"\n",
   ]\n",
  },\n",
  {\n",
   \"cell_type\": \"markdown\",\n",
   \"metadata\": {},\n",
   \"source\": [\n",
    \"    \"## Display Results\"\n",
   ]\n",
  },\n",
  {\n",
   \"cell_type\": \"code\",\n",
   \"execution_count\": null,\n",
   \"metadata\": {},\n",
   \"outputs\": [],\n",
   \"source\": [\n",
    \"    \"# Display evaluation metrics\\n\",\n",
    \"    \"print(\\\"Simulator-Based Evaluation Metrics:\\\")\\n\",\n",
    \"    \"print(\\\"=\\\" * 50)\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"metrics = evaluation_result.get(\\\"metrics\\\", {})\\n\",\n",
    \"    \"for metric_name, metric_value in metrics.items():\\n\",\n",
    \"    \"    print(f\\\"{metric_name}: {metric_value:.4f}\\\")\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"print(\\\"\\\\nDetailed results are available in Azure AI Foundry Studio.\\\")\"\n",
   ]\n",
  },\n",
  {\n",
   \"cell_type\": \"markdown\",\n",
   \"metadata\": {},\n",
   \"source\": [\n",
    \"    \"## Alternative: Conversation Starter Simulation\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"Let's also try generating queries using conversation starters specific to restaurant scenarios.\"\n",
   ]\n",
  },\n",
  {\n",
   \"cell_type\": \"code\",\n",
   \"execution_count\": null,\n",
   \"metadata\": {},\n",
   \"outputs\": [],\n",
   \"source\": [\n",
    \"    \"# Define conversation starters for restaurant scenarios\\n\",\n",
    \"    \"conversation_starters = [\\n\",\n",
    \"    \"    \\\"I'm looking for information about your restaurant\\\",\\n\",\n",
    \"    \"    \\\"I'd like to know about your menu\\\",\\n\",\n",
    \"    \"    \\\"What are your operating hours?\\\",\\n\",\n",
    \"    \"    \\\"I want to make a reservation\\\",\\n\",\n",
    \"    \"    \\\"Tell me about your steaks\\\",\\n\",\n",
    \"    \"    \\\"I have dietary restrictions\\\",\\n\",\n",
    \"    \"    \\\"What's the price range for dinner?\\\",\\n\",\n",
    \"    \"    \\\"I'm celebrating a special occasion\\\",\\n\",\n",
    \"    \"    \\\"How do I contact the restaurant?\\\",\\n\",\n",
    \"    \"    \\\"Tell me about the restaurant's history\\\",\\n\",\n",
    \"    \"]\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"print(\\\"Generating queries from conversation starters...\\\")\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"# Generate more targeted queries\\n\",\n",
    \"    \"targeted_conversations = await simulator.generate_async(\\n\",\n",
    \"    \"    target=query_restaurant_agent_async,\\n\",\n",
    \"    \"    text_inputs=conversation_starters,\\n\",\n",
    \"    \"    num_queries=30,  # Generate 30 targeted queries\\n\",\n",
    \"    \"    max_conversation_turns=1,\\n\",\n",
    \"    \")\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"print(f\\\"Generated {len(targeted_conversations)} targeted conversations!\\\")\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"# Show some examples\\n\",\n",
    \"    \"print(\\\"\\\\nSample Targeted Queries:\\\")\\n\",\n",
    \"    \"print(\\\"-\\\" * 40)\\n\",\n",
    \"    \"for i in range(min(5, len(targeted_conversations))):\\n\",\n",
    \"    \"    conv = targeted_conversations[i]\\n\",\n",
    \"    \"    query = conv.get('query', f'Targeted query {i+1}')\\n\",\n",
    \"    \"    response = conv.get('response', 'No response')\\n\",\n",
    \"    \"    print(f\\\"\\\\n{i+1}. Query: {query}\\\")\\n\",\n",
    \"    \"    print(f\\\"   Response: {response[:150]}{'...' if len(response) > 150 else ''}\\\")\\n\",\n",
    \"    \"    print(\\\"-\\\" * 60)\"\n",
   ]\n",
  },\n",
  {\n",
   \"cell_type\": \"markdown\",\n",
   \"metadata\": {},\n",
   \"source\": [\n",
    \"    \"## Compare Simulation Approaches\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"Let's compare the different approaches to understand their effectiveness.\"\n",
   ]\n",
  },\n",
  {\n",
   \"cell_type\": \"code\",\n",
   \"execution_count\": null,\n",
   \"metadata\": {},\n",
   \"outputs\": [],\n",
   \"source\": [\n",
    \"    \"# Summary of different evaluation approaches\\n\",\n",
    \"    \"print(\\\"Evaluation Approach Comparison:\\\")\\n\",\n",
    \"    \"print(\\\"=\\\" * 50)\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"print(\\\"1. Pre-defined Queries (.jsonl file):\\\")\\n\",\n",
    \"    \"print(\\\"   ✓ Predictable and reproducible\\\")\\n\",\n",
    \"    \"print(\\\"   ✓ Covers specific test cases\\\")\\n\",\n",
    \"    \"print(\\\"   ✓ Good for regression testing\\\")\\n\",\n",
    \"    \"print(\\\"   - Limited diversity\\\")\\n\",\n",
    \"    \"print(\\\"   - Manual creation effort\\\")\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"print(\\\"2. AI Simulator (General Context):\\\")\\n\",\n",
    \"    \"print(\\\"   ✓ High diversity of queries\\\")\\n\",\n",
    \"    \"print(\\\"   ✓ Discovers edge cases\\\")\\n\",\n",
    \"    \"print(\\\"   ✓ Realistic customer language\\\")\\n\",\n",
    \"    \"print(\\\"   - Less predictable\\\")\\n\",\n",
    \"    \"print(\\\"   - May generate irrelevant queries\\\")\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"print(\\\"3. AI Simulator (Conversation Starters):\\\")\\n\",\n",
    \"    \"print(\\\"   ✓ Balanced approach\\\")\\n\",\n",
    \"    \"print(\\\"   ✓ Relevant to specific use cases\\\")\\n\",\n",
    \"    \"print(\\\"   ✓ Natural conversation flow\\\")\\n\",\n",
    \"    \"print(\\\"   ✓ Good for exploratory testing\\\")\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"print(\\\"4. Red Team Evaluation:\\\")\\n\",\n",
    \"    \"print(\\\"   ✓ Tests safety and robustness\\\")\\n\",\n",
    \"    \"print(\\\"   ✓ Adversarial attack detection\\\")\\n\",\n",
    \"    \"print(\\\"   ✓ Compliance and risk assessment\\\")\\n\",\n",
    \"    \"print(\\\"   - Focused on safety, not functionality\\\")\"\n",
   ]\n",
  },\n",
  {\n",
   \"cell_type\": \"markdown\",\n",
   \"metadata\": {},\n",
   \"source\": [\n",
    \"    \"## Summary and Recommendations\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"This simulator-based evaluation notebook demonstrates:\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"### Key Features:\\n\",\n",
    \"    \"1. **AI-Generated Queries**: Uses Azure AI to generate realistic customer queries\\n\",\n",
    \"    \"2. **Context-Aware**: Generates queries relevant to restaurant scenarios\\n\",\n",
    \"    \"3. **Diverse Testing**: Discovers edge cases and unexpected query patterns\\n\",\n",
    \"    \"4. **Natural Language**: Tests with realistic customer language and phrasing\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"### Benefits:\\n\",\n",
    \"    \"- **Scalable**: Can generate hundreds of test queries automatically\\n\",\n",
    \"    \"- **Dynamic**: Each run produces different queries for comprehensive testing\\n\",\n",
    \"    \"- **Realistic**: Simulates actual customer interactions\\n\",\n",
    \"    \"- **Exploratory**: Helps discover unexpected agent behaviors\\n\",\n",
    \"    \"\\n\",\n",
    \"    \"### Best Practices:\\n\",\n",
    \"    \"1. **Combine Approaches**: Use both pre-defined and simulated queries\\n\",\n",
    \"    \"2. **Regular Testing**: Run simulations periodically to catch regressions\\n\",\n",
    \"    \"3. **Context Refinement**: Adjust context based on discovered issues\\n\",\n",
    \"    \"4. **Metric Monitoring**: Track evaluation scores over time\\n\",\n",
    \"    \"5. **Manual Review**: Review generated queries and responses for quality\"\n",
   ]\n",
  }\n",
 ],\n",
 \"metadata\": {\n",
  \"kernelspec\": {\n",
   \"display_name\": \"Python 3\",\n",
   \"language\": \"python\",\n",
   \"name\": \"python3\"\n",
  },\n",
  \"language_info\": {\n",
   \"codemirror_mode\": {\n",
    \"name\": \"ipython\",\n",
    \"version\": 3\n",
   },\n",
   \"file_extension\": \".py\",\n",
   \"mimetype\": \"text/x-python\",\n",
   \"name\": \"python\",\n",
   \"nbconvert_exporter\": \"python\",\n",
   \"pygments_lexer\": \"ipython3\",\n",
   \"version\": \"3.12.11\"\n",
  }\n",
 },\n",
 \"nbformat\": 4,\n",
 \"nbformat_minor\": 4\n",
}"
